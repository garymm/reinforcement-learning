from typing import Iterator, Optional

import jax
import jax.numpy as jnp
import jaxtyping
import numpy as np
import reverb
from corax import adders, core, specs
from corax.adders import reverb as adders_reverb
from corax.agents.jax import actor_core as actor_core_lib
from corax.agents.jax import actors, builders
from corax.datasets import reverb as datasets_reverb
from corax.jax import networks as networks_lib
from corax.jax import types as jax_types
from corax.jax import utils, variable_utils
from reverb import rate_limiters

from rl.agents.jax.dqn.config import DQNConfig
from rl.agents.jax.dqn.networks import (
    DQNNetworks,
)

_REPLAY_TABLE_NAME = "replay"


class DQNBuilder(
    builders.ActorLearnerBuilder[
        DQNNetworks, actor_core_lib.FeedForwardPolicy, reverb.ReplaySample
    ]
):
    def __init__(self, config: DQNConfig):
        self._config = config

    def make_replay_tables(
        self,
        environment_spec: specs.EnvironmentSpec,
        policy: actor_core_lib.FeedForwardPolicy,
    ) -> list[reverb.Table]:
        """Create tables to insert data into.

        Args:
          environment_spec: A container for all relevant environment specs.
          policy: Agent's policy which can be used to extract the extras_spec.

        Returns:
          The replay tables used to store the experience the agent uses to train.
        """
        signature = adders_reverb.SequenceAdder.signature(environment_spec)
        return [
            reverb.Table(
                name=_REPLAY_TABLE_NAME,
                sampler=reverb.selectors.Uniform(),
                remover=reverb.selectors.Fifo(),
                max_size=self._config.replay_buffer_size,
                rate_limiter=rate_limiters.Queue(self._config.replay_buffer_size),
                signature=signature,
            )
        ]

    def make_dataset_iterator(
        self,
        replay_client: reverb.Client,
    ) -> Iterator[reverb.ReplaySample]:
        """Create a dataset iterator to use for learning/updating the agent."""
        dataset = datasets_reverb.make_reverb_dataset(
            table=_REPLAY_TABLE_NAME,
            server_address=replay_client.server_address,
            batch_size=self._config.batch_size,
        )
        return utils.device_put(dataset.as_numpy_iterator(), jax.devices()[0])

    def make_adder(
        self,
        replay_client: "reverb.Client",
        environment_spec: Optional[specs.EnvironmentSpec],
        policy: Optional[builders.Policy],
    ) -> Optional[adders.Adder]:
        """Create an adder which records data generated by the actor/environment.

        Args:
          replay_client: Reverb Client which points to the replay server.
          environment_spec: specs of the environment.
          policy: Agent's policy which can be used to extract the extras_spec.
        """
        pass  # TODO

    def make_actor(
        self,
        random_key: networks_lib.PRNGKey,
        policy: actor_core_lib.FeedForwardPolicy,
        environment_spec: specs.EnvironmentSpec,
        variable_source: Optional[core.VariableSource] = None,
        adder: Optional[adders.Adder] = None,
    ) -> core.Actor:
        assert variable_source
        actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)
        variable_client = variable_utils.VariableClient(
            variable_source,
            "params",
            update_period=self._config.variable_update_period,
            device=self._config.device,
        )
        return actors.GenericActor(
            actor_core, random_key, variable_client, adder, backend=self._config.device
        )

    def make_learner(
        self,
        random_key: networks_lib.PRNGKey,
        networks: DQNNetworks,
        dataset: Iterator[Sample],
        logger_fn: loggers.LoggerFactory,
        environment_spec: specs.EnvironmentSpec,
        replay_client: Optional["reverb.Client"] = None,
        counter: Optional[counting.Counter] = None,
    ) -> core.Learner:
        """Creates an instance of the learner.

        Args:
          random_key: A key for random number generation.
          networks: struct describing the networks needed by the learner; this can
            be specific to the learner in question.
          dataset: iterator over samples from replay.
          logger_fn: factory providing loggers used for logging progress.
          environment_spec: A container for all relevant environment specs.
          replay_client: client which allows communication with replay. Note that
            this is only intended to be used for updating priorities. Samples should
            be obtained from `dataset`.
          counter: a Counter which allows for recording of counts (learner steps,
            actor steps, etc.) distributed throughout the agent.
        """
        pass  # TODO

    def make_policy(
        self,
        networks: DQNNetworks,
        environment_spec: specs.EnvironmentSpec,
        evaluation: bool = False,
    ) -> actor_core_lib.FeedForwardPolicy:
        assert networks.q_network

        def _greedy_policy(
            params: DQNNetworks,
            key: jax_types.PRNGKey,
            obs: jaxtyping.Array | np.ndarray,
        ) -> jaxtyping.Array:
            q = networks.q_network.apply(params, obs, is_training=True)
            return jnp.argmax(q)

        def _epsilon_greedy_policy(
            params: DQNNetworks,
            key: jax_types.PRNGKey,
            obs: jaxtyping.Array | np.ndarray,
        ) -> jaxtyping.Array:
            # From the paper algorithm 1, epsilon greedy policy.
            if jax.random.uniform(key) < self._config.epsilon:
                return jax.random.randint(
                    key, (1,), 0, environment_spec.actions.shape[0]
                )
            return _greedy_policy(params, key, obs)

        if evaluation:
            return _greedy_policy
        return _epsilon_greedy_policy
