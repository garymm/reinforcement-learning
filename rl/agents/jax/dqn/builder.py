from typing import Iterator, Optional

import reverb
from corax import adders, core, specs
from corax.agents.jax import actor_core as actor_core_lib
from corax.agents.jax import actors, builders
from corax.jax import networks as networks_lib

from rl.agents.jax.dqn.config import DQNConfig
from rl.agents.jax.dqn.networks import (
    CallableFeedForwardPolicy,
    DQNNetworks,
    make_policy,
)


class DQNBuilder(
    builders.ActorLearnerBuilder[DQNNetworks, CallableFeedForwardPolicy, TODO]
):
    def __init__(self, config: DQNConfig):
        self._config = config

    def make_replay_tables(
        self,
        environment_spec: specs.EnvironmentSpec,
        policy: builders.Policy,
    ) -> list[reverb.Table]:
        """Create tables to insert data into.

        Args:
          environment_spec: A container for all relevant environment specs.
          policy: Agent's policy which can be used to extract the extras_spec.

        Returns:
          The replay tables used to store the experience the agent uses to train.
        """
        pass  # TODO

    def make_dataset_iterator(
        self,
        replay_client: reverb.Client,
    ) -> Iterator[builders.Sample]:
        """Create a dataset iterator to use for learning/updating the agent."""
        pass  # TODO

    def make_adder(
        self,
        replay_client: "reverb.Client",
        environment_spec: Optional[specs.EnvironmentSpec],
        policy: Optional[builders.Policy],
    ) -> Optional[adders.Adder]:
        """Create an adder which records data generated by the actor/environment.

        Args:
          replay_client: Reverb Client which points to the replay server.
          environment_spec: specs of the environment.
          policy: Agent's policy which can be used to extract the extras_spec.
        """
        pass  # TODO

    def make_actor(
        self,
        random_key: networks_lib.PRNGKey,
        policy: CallableFeedForwardPolicy,
        environment_spec: specs.EnvironmentSpec,
        variable_source: Optional[core.VariableSource] = None,
        adder: Optional[adders.Adder] = None,
    ) -> core.Actor:
        # TODO: this doesn't work because my policy does not take in parameters.
        actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)
        return actors.GenericActor(
            actor_core, random_key, variable_client, adder, backend=device
        )

    def make_learner(
        self,
        random_key: networks_lib.PRNGKey,
        networks: DQNNetworks,
        dataset: Iterator[Sample],
        logger_fn: loggers.LoggerFactory,
        environment_spec: specs.EnvironmentSpec,
        replay_client: Optional["reverb.Client"] = None,
        counter: Optional[counting.Counter] = None,
    ) -> core.Learner:
        """Creates an instance of the learner.

        Args:
          random_key: A key for random number generation.
          networks: struct describing the networks needed by the learner; this can
            be specific to the learner in question.
          dataset: iterator over samples from replay.
          logger_fn: factory providing loggers used for logging progress.
          environment_spec: A container for all relevant environment specs.
          replay_client: client which allows communication with replay. Note that
            this is only intended to be used for updating priorities. Samples should
            be obtained from `dataset`.
          counter: a Counter which allows for recording of counts (learner steps,
            actor steps, etc.) distributed throughout the agent.
        """
        pass  # TODO

    def make_policy(
        self,
        networks: DQNNetworks,
        environment_spec: specs.EnvironmentSpec,
        evaluation: bool = False,
    ) -> CallableFeedForwardPolicy:
        return make_policy(
            networks, environment_spec.actions, self._config.policy_epsilon
        )
